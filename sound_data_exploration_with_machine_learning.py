# -*- coding: utf-8 -*-
"""Sound Data Exploration with Machine Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eYlRrU3KjWyXdGryo4saMGqVh2fEhUy7

# Sound Dataset Exploration

## We Install Packages

Then Install:
- Audio processing: `librosa`, `mutagen`
- Plots: `Plotly`, `matplotlib`
"""

!pip install pandas
!pip install librosa
!pip install plotly
!pip install matplotlib
!pip install mutagen
!pip install pillow

import os
import time
import librosa
import zipfile
import mutagen
import mutagen.wave
import numpy as np
import pandas as pd
import librosa.display
import IPython.display
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from PIL import Image

"""## Data Processing"""

# Unzip dataset
!wget https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz -O urban8k.tgz
!tar -xzf urban8k.tgz
!rm urban8k.tgz

!cat UrbanSound8K/UrbanSound8K_README.txt

"""## Analysis of Data

We'll go through a series of data analysis procedures to look for trends in the provided data and select the most appropriate models in light of those findings.



### Informational Statistics

We are interested in seeing how the various classes are divided up and how the audio files were captured.  





"""

def create_dataset_df(csv_file):
    dataset_df = pd.read_csv(csv_file)
    filepaths = []
    for i, row in dataset_df.iterrows():
        filepaths.append(os.path.join('UrbanSound8K/audio', 'fold'+str(row['fold']), row['slice_file_name']))
    dataset_df['filepath'] = filepaths
    return dataset_df

dataset_df = create_dataset_df('UrbanSound8K/metadata/UrbanSound8K.csv')
dataset_df.head()

dataset_df.groupby('class').slice_file_name.count()

"""All the classes have 1000 samples, with the exception of `car_horn`, `gun_shot` and `siren`. In the event that we observe that the underrepresented classes are not being categorized as well as the others, we may investigate the possibility of adding **class weights** to the loss function or **oversampling**.

We then calculate the statistics for the audio files.
"""

def get_audio_metadata_mutagen(filepath):
    metadata = {}
    f = mutagen.wave.WAVE(filepath)
    metadata['length'] = f.info.length
    metadata['bitrate'] = f.info.bitrate
    metadata['channels'] = f.info.channels
    metadata['sample_rate'] = f.info.sample_rate
    metadata['bits_per_sample'] = f.info.bits_per_sample
    return metadata


def compute_audio_statistics(dataset_df):
    metadata_dict = {'length': [], 'bitrate': [], 'channels': [], 'sample_rate': [], 'bits_per_sample': []}
    # Extract metadata
    for filepath in dataset_df['filepath']:
        metadata = get_audio_metadata_mutagen(filepath)
        for key in metadata_dict.keys():
            metadata_dict[key].append(metadata[key])
    # Add new columns to dataframe
    for key in metadata_dict.keys():
        dataset_df[key] = metadata_dict[key]

    return dataset_df

dataset_df = dataset_df.drop(columns=['fold', 'slice_file_name', 'fsID', 'start', 'end'])

audio_statistics_df = compute_audio_statistics(dataset_df)

"""The following tables show us how the data has been recorded and digitalized in various ways.

- Nearly all of the samples have been recorded primarily utilizing two channels (stereo).
- The sample rates (mostly 44 kHz and 48 kHz) range from 8 kHz to 192 kHz.
- The audios range in length from 0.0008 seconds to 4 seconds (usually 4 seconds).
- There are 4 to 32 bits (usually 24 bits) used each sample.

Before the data is entered into a machine learning model, it must be normalized.

"""

audio_statistics_df.describe()

audio_statistics_df['sample_rate'].value_counts(), audio_statistics_df['bits_per_sample'].value_counts()

"""Upon examining the audio data for each of the various labels, we can observe that:

- The audios for courses 1 and 6 are substantially shorter than those for the other classes. Possibly utilized as a feature if desired.

- No trends are discernible in the remaining metadata, which is very consistent for each class.

"""

audio_statistics_df.groupby('class').describe()

"""### Visualization of Data
Since the data is made up of audio files, we will load, plot, and process the audio using the `Librosa` library.

The `IPython.display.Audio` package will be utilized to enable direct file listening in the Jupyter Notebook.

### Waveform

Initially, we can view the waveform for various random samples of every class:
"""

!pip install matplotlib librosa
!pip install --upgrade librosa matplotlib
!pip install librosa matplotlib

import librosa
import librosa.display
import matplotlib.pyplot as plt

# Randomly select one sample of each class
random_samples = dataset_df.groupby('class').sample(1)
audio_samples, labels = random_samples['filepath'].tolist(), random_samples['class'].tolist()

# Visualize the waveforms
fig, axs = plt.subplots(5, 2, figsize=(15, 15))
index = 0
for col in range(2):
    for row in range(5):
        audio_file, sample_rate = librosa.load(audio_samples[index])
        librosa.display.waveshow(y=audio_file, sr=sample_rate, ax=axs[row][col])
        axs[row][col].set_title('{}'.format(labels[index]))
        index += 1

fig.tight_layout()
plt.show()

"""MFCC (Mel-Frequency Cepstral Coefficients), STFT (Short-Term Fourier Transform), and Mel-Spectograms are three widely used techniques for processing audio signals and producing features that are fed into machine learning algorithms like convolutional neural networks.

### STFT (Log Y Axis)
"""

import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np

# Visualize the STFT
n_fft = 2048
hop_length = 512
fig, axs = plt.subplots(5, 2, figsize=(20, 20))
index = 0
n_s = 4

for col in range(2):
    for row in range(5):
        audio_file, sample_rate = librosa.load(audio_samples[index])
        stft = librosa.stft(y=audio_file, n_fft=n_fft, hop_length=hop_length)  # STFT of y
        S_db = librosa.amplitude_to_db(np.abs(stft), ref=np.max)
        librosa.display.specshow(S_db,
                                 sr=sample_rate,
                                 hop_length=hop_length,
                                 x_axis="time",
                                 y_axis='log',
                                 ax=axs[row][col])
        axs[row][col].set_title('{}'.format(labels[index]))
        index += 1

fig.tight_layout()
plt.show()

"""### Mel-Spectogram

The Mel-Spectogram, which shows the various frequency magnitudes at various timesteps, will then be visualized. The frequency magnitude is converted to a form on the Mel Scale, which considers the perception and processing of audio signals by humans.
"""

import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np

# Visualize the Mel-Spectrograms
n_fft = 2048
hop_length = 512
fig, axs = plt.subplots(5, 2, figsize=(20, 20))
index = 0
n_s = 4

for col in range(2):
    for row in range(5):
        audio_file, sample_rate = librosa.load(audio_samples[index])
        stft = librosa.stft(y=audio_file, n_fft=n_fft, hop_length=hop_length)  # STFT of y
        S_db = librosa.amplitude_to_db(np.abs(stft), ref=np.max)
        mel_spec = librosa.feature.melspectrogram(S=np.abs(stft)**2, sr=sample_rate)
        S_db_mel = librosa.power_to_db(mel_spec, ref=np.max)
        librosa.display.specshow(S_db_mel,
                                 sr=sample_rate,
                                 hop_length=hop_length,
                                 x_axis="time",
                                 y_axis='mel',
                                 ax=axs[row][col])
        axs[row][col].set_title('{}'.format(labels[index]))
        index += 1

fig.tight_layout()
plt.show()

# Listen to the recordings (index can be changed to listen to a different recording)
index = 0
print('Listen to {} sample'.format(labels[index]))
IPython.display.Audio(audio_samples[index])

audio_file, sample_rate = librosa.load(audio_samples[0])
stft = librosa.stft(audio_file)  # STFT of y
S_db = librosa.amplitude_to_db(np.abs(stft), ref=np.max)
print(S_db.shape)
S_db

"""### MFCCs"""

import librosa
import librosa.display
import matplotlib.pyplot as plt

# Visualize 40 MFCCs
n_fft = 2048
hop_length = 512
fig, axs = plt.subplots(5, 2, figsize=(20, 20))
index = 0
n_s = 4

for col in range(2):
    for row in range(5):
        audio_file, sample_rate = librosa.load(audio_samples[index])
        mfccs = librosa.feature.mfcc(y=audio_file,
                                     sr=sample_rate,
                                     n_fft=n_fft,
                                     hop_length=hop_length,
                                     n_mfcc=40)
        librosa.display.specshow(mfccs,
                                 sr=sample_rate,
                                 hop_length=hop_length,
                                 x_axis="time",
                                 ax=axs[row][col])
        axs[row][col].set_title('{}'.format(labels[index]))
        index += 1

fig.tight_layout()
plt.show()

"""We can see that the signals for the dataset classes are sufficiently different to be correctly classified after looking at the graphs for a few different permutations.

From this point on, we can process the data and categorize the various audio files using machine learning algorithms.

## Applying Machine Learning on Sound Data
# Install Packages
**We install:**

Machine learning libraries: Keras, sklearn

Audio processing: librosa

Plots: Plotly, matplotlib
"""

!pip install pandas
!pip install setuptools
!pip install numpy
!pip install sklearn
!pip install librosa
!pip install plotly
!pip install matplotlib
!pip install pillow
!pip install keras

!pip install --upgrade pip
!pip install numpy scipy
!pip install scikit-learn

import os
import time
import librosa
import zipfile
import numpy as np
import pandas as pd
import librosa.display
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from PIL import Image

# Unzip dataset
!wget https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz -O urban8k.tgz
!tar -xzf urban8k.tgz
!rm urban8k.tgz

"""## Design Choices and Models
I have made the following design decisions and recommendations after analyzing the dataset, reading up on the state-of-the-art in audio signal classification, and reviewing some of my earlier work:




Convolutional neural networks can be trained with input from MFCCs, STFTs, or Mel-Spectograms.


* I pad the spectogram that is generated because the audio durations range from 0 to 4 seconds, so each audio is the same length.




Features available:

* Employing MFCCs as attributes:

  * The first 13 MFCCs, their derivatives, and their second derivatives are typically computed and used as features. Alternatively, using 40 MFCCs is customary as it is the Librosa default.

* Using the features of the STFT:

  * Has less human processing than Mel-Spectograms and MFCCs; instead of learning human-designed representations, the CNN might learn other filters.

* Using features from the Mel-Spectogram:

  * A change made to the STFT that simulates how people would hear sound. A little bit more than STFT, but less human engineered than **MFCCs**.


Since it appears that CNNs could benefit more from the frequency-temporal structure, my first choice would be to use STFT and Mel-Spectogram. However, because of **computational resources** and time constraints, I will demonstrate the use of MFCCs as features because they are significantly more memory efficient.

## Splitting and Preprocessing of Datasets
I use Librosa to load all of the audio data at the factory default sample rate of 22050Hz. Source served as the basis for this design choice, though other sample rates might be tested in subsequent studies.


> Since humans have a maximum hearing range of about 20,000 hertz, it is feasible to analyze speech and music data effectively and affordably at much lower frequencies. The highest pitches that typically concern us are approximately C9â‰ˆ8372 Hz, which is significantly lower than the 11025 cutoff that fs=22050 suggests.




Librosa loads the audio in mono by default, providing us with one channel.
"""

# FeatureExtractor class including librosa audio processing functions
class FeatureExtractor:
    def __init__(self, csv_file):
        self.csv_file = csv_file
        self.max_audio_duration = 4
        self.dataset_df = self._create_dataset(csv_file)

    @staticmethod
    def _create_dataset(csv_file):
        """
        Args:
            dataset_path: path with the .wav files after unzipping
        Returns: A pandas dataframe with the list of files and labels (`filenames`, `labels`)
        """
        dataset_df = pd.read_csv(csv_file)
        filepaths = []
        for i, row in dataset_df.iterrows():
            filepaths.append(os.path.join('UrbanSound8K/audio', 'fold'+str(row['fold']), row['slice_file_name']))
        dataset_df['filepath'] = filepaths
        return dataset_df

    @staticmethod
    def _compute_max_pad_length(max_audio_length, sample_rate=22050, n_fft=2048, hop_length=512):
        dummy_file = np.random.random(max_audio_length*sample_rate)
        stft = librosa.stft(dummy_file, n_fft=n_fft, hop_length=hop_length)
        # Return an even number for CNN computation purposes
        if stft.shape[1] % 2 != 0:
            return stft.shape[1] + 1
        return stft.shape[1]

    def compute_save_features(self,
                        mode='mfcc',
                        sample_rate=22050,
                        n_fft=2048,
                        hop_length=512,
                        n_mfcc=40,
                        output_path='features',
                        deltas=False
                        ):
        dataset_features = []
        max_pad = self._compute_max_pad_length(self.max_audio_duration,
                                               sample_rate=sample_rate,
                                               n_fft=n_fft,
                                               hop_length=hop_length)
        print('Max Padding = ', max_pad)

        if not os.path.exists(output_path):
            print('Creating output folder: ', output_path)
            os.makedirs(output_path)
        else:
            print('Output folder already existed')

        print('Saving features in ', output_path)
        i = 0
        t = time.time()

        features_path = []
        for filepath in self.dataset_df['filepath']:
            if i % 100 == 0:
                print('{} files processed in {}s'.format(i, time.time() - t))
            audio_file, sample_rate = librosa.load(filepath, sr=sample_rate, res_type='kaiser_fast')
            if mode == 'mfcc':
                audio_features = self.compute_mfcc(audio_file, sample_rate, n_fft, hop_length, n_mfcc, deltas)
            elif mode == 'stft':
                audio_features = self.compute_stft(audio_file, sample_rate, n_fft, hop_length)
            elif mode == 'mel-spectogram':
                audio_features = self.compute_mel_spectogram(audio_file, sample_rate, n_fft, hop_length)

            audio_features = np.pad(audio_features,
                                    pad_width=((0, 0), (0, max_pad - audio_features.shape[1])))

            save_path = os.path.join(output_path, filepath.split('/')[-1].replace('wav', 'npy'))
            self.save_features(audio_features, save_path)
            features_path.append(save_path)
            i+=1
        self.dataset_df['features_path'] = features_path
        return self.dataset_df

    @staticmethod
    def save_features(audio_features, filepath):
        np.save(filepath, audio_features)

    @staticmethod
    def compute_mel_spectogram(audio_file, sample_rate, n_fft, hop_length):
        return librosa.feature.melspectrogram(audio_file,
                                              sr=sample_rate,
                                              n_fft=n_fft,
                                              hop_length=hop_length)
    @staticmethod
    def compute_stft(audio_file, sample_rate, n_fft, hop_length):
        return librosa.stft(audio_file, n_fft=n_fft, hop_length=hop_length)

    @staticmethod
    def compute_mfcc(audio_file, sample_rate, n_fft, hop_length, n_mfcc, deltas=False):
        mfccs = librosa.feature.mfcc(audio_file,
                                    sr=sample_rate,
                                    n_fft=n_fft,
                                    n_mfcc=n_mfcc,
                                    )
        # Change mode from interpolation to nearest
        if deltas:
          delta_mfccs = librosa.feature.delta(mfccs, mode='nearest')
          delta2_mfccs = librosa.feature.delta(mfccs, order=2, mode='nearest')
          return np.concatenate((mfccs, delta_mfccs, delta2_mfccs))
        return mfccs

# Create dataset and extract features
fe = FeatureExtractor('UrbanSound8K/metadata/UrbanSound8K.csv')

# Unzip features
!wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1BU2B5EcbfyGBIOkB5YC44hpzPpuqw43H' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1BU2B5EcbfyGBIOkB5YC44hpzPpuqw43H" -O features_mfcc.zip && rm -rf /tmp/cookies.txt
!unzip -q features_mfcc.zip
!rm features_mfcc.zip

# Download dataset.json file
!wget --no-check-certificate "https://docs.google.com/uc?export=download&id=1pzSvGYaBXghLQFTZxlSex-Ts3T4B0X4C" -O dataset.json

dataset_df = pd.read_json('dataset.json')

"""We will load all of the data into memory and process it in minibatches for the purposes of this experiment. We could create Dataloader objects, which would enable us to execute numerous other operations like Data Augmentation and iterate more quickly, if we had more time and computational power.



"""

dataset_df['features'] = [np.asarray(np.load(feature_path)) for feature_path in dataset_df['features_path']]

from keras.utils import to_categorical
dataset_df['labels_categorical'] = [to_categorical(label, 10) for label in dataset_df['classID']]

dataset_df.head()

"""We are going to create splits for the train, validation and test sets of our dataset. For the purpose of the experiment and to make it quick we will use the sklearn function **train_test_split**, two times.


"""

# Split the dataset
from sklearn.model_selection import train_test_split

# Add one dimension for the channel
X = np.array(dataset_df['features'].tolist())
y = np.array(dataset_df['labels_categorical'].tolist())

# As there is unbalance for some classes I am going to stratify it so we have the same proportion in train/test
X_train, X_test, Y_train, Y_test = train_test_split(X,
                                                    y,
                                                    test_size=0.30,
                                                    random_state=1,
                                                    stratify=y)
# Create validation and test
X_test, X_val, Y_test, Y_val = train_test_split(X_test,
                                                Y_test,
                                                test_size=0.5,
                                                random_state=1,
                                                stratify=Y_test)

print(X_train.shape, X_val.shape, X_test.shape)

"""## Machine Learning Model

#Model Design
We are going to use Keras running over Tensorflow with several layers to create a **Fully Convolutional Network Model**.
"""

from keras.models import Sequential
from keras.layers import Input, Dense, Dropout, Activation, Flatten
from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D

"""Since our images are rectangular in shape (time is on the x axis, and the y axis represents the MFCC), we will use rectangular filters rather than the more common square filters to improve the way the MFCCs correlate with time.

"""

# FCN Model
def create_model(num_classes=10, input_shape=None, dropout_ratio=None):
    model = Sequential()
    if input_shape is None:
        model.add(Input(shape=(None, None, 1)))
    else:
        model.add(Input(shape=input_shape))
    model.add(Conv2D(filters=16, kernel_size=(2, 4), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 3)))
    model.add(Conv2D(filters=32, kernel_size=(2, 4), activation='relu'))
    model.add(MaxPooling2D(pool_size=2))
    model.add(Conv2D(filters=64, kernel_size=(2, 4), activation='relu'))
    model.add(MaxPooling2D(pool_size=2))
    model.add(Conv2D(filters=128, kernel_size=(2, 4), activation='relu'))
    model.add(GlobalAveragePooling2D())
    if dropout_ratio is not None:
        model.add(Dropout(dropout_ratio))
    # Add dense linear layer
    model.add(Dense(num_classes, activation='softmax'))
    return model

"""We will apply the Categorical Cross Entropy loss because it is a multiclass problem. We will use Adam's Keras implementation as the optimizer, using the default values for the hyperparameters.

"""

# Create and compile the model
fcn_model = create_model(input_shape=X_train.shape[1:])
fcn_model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')
fcn_model.summary()

"""# Model training and evaluation"""

from keras.models import load_model
from keras.callbacks import ModelCheckpoint

def train_model(model, X_train, Y_train, X_val, Y_val, epochs, batch_size, callbacks):
    model.fit(X_train,
              Y_train,
              batch_size=batch_size,
              epochs=epochs,
              validation_data=(X_val, Y_val),
              callbacks=callbacks, verbose=1)
    return model

"""The model that performs better on the validation set will be chosen to serve as our checkpoint for **early stopping**.

Our ability to perform hyperparameter tuning more quickly will come from building a function to train the model.

"""

checkpointer = ModelCheckpoint(filepath='saved_models/best_fcn.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True)
callbacks = [checkpointer]

# Hyper-parameters
epochs = 100
batch_size = 256

# Train the model
model = train_model(model=fcn_model,
                    X_train=X_train,
                    X_val=X_val,
                    Y_train=Y_train,
                    Y_val=Y_val,
                    epochs=epochs,
                    batch_size=batch_size,
                    callbacks=callbacks)

# Load the best model
best_model = load_model('saved_models/best_fcn.hdf5')

"""Looks like the model has overfitted to the training data towards the end of the training. We have selected the model that performed better on the validation set, saved by the checkpoint. The similarity between validation and test score tells us that our training methodology is correct and that our validation set is a good estimator of testing performance."""

# Evaluating the model on the training and testing set
score = best_model.evaluate(X_train, Y_train, verbose=0)
print("Training Accuracy: ", score[1])

score = best_model.evaluate(X_val, Y_val, verbose=0)
print("Validation Accuracy: ", score[1])

score = best_model.evaluate(X_test, Y_test, verbose=0)
print("Testing Accuracy: ", score[1])

"""We see that there has been overfitting so we could train another model adding dropout before the last layer to add more regularization.


"""

# We add a dropout ratio of 0.25
fcn_model = create_model(input_shape=X_train.shape[1:], dropout_ratio=0.5)
fcn_model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')
fcn_model.summary()

checkpointer = ModelCheckpoint(filepath='saved_models/best_fcn_dropout.hdf5', monitor='val_accuracy',
                               verbose=1, save_best_only=True)
callbacks = [checkpointer]

model = train_model(model=fcn_model,
                    X_train=X_train,
                    X_val=X_val,
                    Y_train=Y_train,
                    Y_val=Y_val,
                    epochs=200,
                    batch_size=256,
                    callbacks=callbacks)

best_model = load_model('saved_models/best_fcn_dropout.hdf5')

# Evaluating the model on the training and testing set
score = best_model.evaluate(X_train, Y_train, verbose=0)
print("Training Accuracy: ", score[1])

score = best_model.evaluate(X_val, Y_val, verbose=0)
print("Validation Accuracy: ", score[1])

score = best_model.evaluate(X_test, Y_test, verbose=0)
print("Testing Accuracy: ", score[1])

# Plot a confusion matrix
from sklearn import metrics
Y_pred = best_model.predict(X_test)
matrix = metrics.confusion_matrix(Y_test.argmax(axis=1), Y_pred.argmax(axis=1))

# Confusion matrix code (from https://github.com/triagemd/keras-eval/blob/master/keras_eval/visualizer.py)
def plot_confusion_matrix(cm, concepts, normalize=False, show_text=True, fontsize=18, figsize=(16, 12),
                          cmap=plt.cm.coolwarm_r, save_path=None, show_labels=True):
    '''
    Plot confusion matrix provided in 'cm'
    Args:
        cm: Confusion Matrix, square sized numpy array
        concepts: Name of the categories to show
        normalize: If True, normalize values between 0 and ones. Not valid if negative values.
        show_text: If True, display cell values as text. Otherwise only display cell colors.
        fontsize: Size of text
        figsize: Size of figure
        cmap: Color choice
        save_path: If `save_path` specified, save confusion matrix in that location
    Returns: Nothing. Plots confusion matrix
    '''

    if cm.ndim != 2 or cm.shape[0] != cm.shape[1]:
        raise ValueError('Invalid confusion matrix shape, it should be square and ndim=2')

    if cm.shape[0] != len(concepts) or cm.shape[1] != len(concepts):
        raise ValueError('Number of concepts (%i) and dimensions of confusion matrix do not coincide (%i, %i)' %
                         (len(concepts), cm.shape[0], cm.shape[1]))

    plt.rcParams.update({'font.size': fontsize})

    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    if normalize:
        cm = cm_normalized

    fig = plt.figure(figsize=figsize)
    ax = fig.add_subplot(111)
    cax = ax.matshow(cm, vmin=np.min(cm), vmax=np.max(cm), alpha=0.8, cmap=cmap)

    fig.colorbar(cax)
    ax.xaxis.tick_bottom()
    plt.ylabel('True label', fontweight='bold')
    plt.xlabel('Predicted label', fontweight='bold')

    if show_labels:
        n_labels = len(concepts)
        ax.set_xticklabels(concepts)
        ax.set_yticklabels(concepts)
        plt.xticks(np.arange(0, n_labels, 1.0), rotation='vertical')
        plt.yticks(np.arange(0, n_labels, 1.0))
    else:
        plt.axis('off')

    if show_text:
        # http://stackoverflow.com/questions/21712047/matplotlib-imshow-matshow-display-values-on-plot
        min_val, max_val = 0, len(concepts)
        ind_array = np.arange(min_val, max_val, 1.0)
        x, y = np.meshgrid(ind_array, ind_array)
        for i, (x_val, y_val) in enumerate(zip(x.flatten(), y.flatten())):
            c = cm[int(x_val), int(y_val)]
            ax.text(y_val, x_val, c, va='center', ha='center')

    if save_path is not None:
        plt.savefig(save_path)

"""Plotting the confusion matrix helps us observe the model's performance and errors made across classes more clearly.

Since the dataset is largely balanced in our case, accuracy is a useful metric. However, we noticed that some classes (**1car_horn, gun_shot, and siren**) have fewer samples, so it will be interesting to see how these classes perform.

Since many errors are occurring between classes **children_playing and class street_music**, it might be worthwhile to invest a little more time in conducting analysis and determining potential causes.

"""

class_dictionary = {3: 'dog_bark', 2: 'children_playing', 1: 'car_horn', 0: 'air_conditioner', 9: 'street_music', 6: 'gun_shot', 8: 'siren', 5: 'engine_idling', 7: 'jackhammer', 4: 'drilling'}
classes = [class_dictionary[key] for key in sorted(class_dictionary.keys())]

plot_confusion_matrix(matrix, classes)

"""### Final Thoughts
The test set accuracy shows a 1-2% increase when dropout is added as regularization. This demonstrates that adding it to our model has been successful.

We can attempt a variety of things to enhance the model's performance, including:

* Adjusting hyperparameters:

  * adjusting feature extraction parameters;
  * adjusting network parameters (layer count, pooling layer count, number and filter shape, etc.); * adjusting network hyperparameters (learning rate, optimizer)

* Extraction of features:

  * Use STFT: Compared to MFCCs, the raw spectogram may give the CNN more information to understand the relationship between frequency and time.
  * Use the Mel-Spectogram: Compared to the MFCCs, the Mel-Spectogram may give the CNN more information to understand the relationship between frequency and time.
"""